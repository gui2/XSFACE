\documentclass[10pt,letterpaper]{article} 

\usepackage{cogsci} 
\usepackage{pslatex} 
\usepackage{graphicx}
\usepackage{pdfsync}
\usepackage{apacite}
\usepackage[raggedright]{sidecap}


\title{Developmental changes in children's visual access to faces during early word learning}

% \author{{\large \bf Michael C. Frank} \\ \texttt{mcfrank@stanford.edu} \\ Department of Psychology \\ Stanford University \And
% {\large \bf Kaia Simmons} \\ \texttt{kaias@stanford.edu} \\ Program in Human Biology \\ Stanford University 
% \And {\large \bf Daniel Yurovsky} \\ \texttt{dyurovsky@stanford.edu} \\ Department of Psychology \\ Stanford University
% \And {\large \bf Guido Pusiol} \\ \texttt{pusiol@stanford.edu} \\ Department of Psychology \\ Stanford University}

\author{{\large \bf Michael C. Frank, Kaia Simmons, Daniel Yurovsky, \& Guido Pusiol} \\
\texttt{\{mcfrank,kaias,yurovsky,pusiol\}@stanford.edu} \\
Department of Psychology \\
Stanford University}

\begin{document}

\maketitle

\begin{abstract} 

The faces of other people are a critical information source for young word learners. Yet during the period of early word learning, children are also undergoing significant postural and locomotor development, changing from lying and sitting infants to toddlers. We used a head-mounted camera in conjunction with a face-detection system to explore the effects of these changes on children's visual access to their caregivers' faces during an in-lab play session. In a cross-sectional sample of 4--20 month old children playing with their caregivers, we found substantial changes in face accessibility based on age and posture. These changes translate into changes in the accessibility of social information during word learning. 

{Keywords:} Social development; face processing; word learning; head-camera 
\end{abstract}

\section{Introduction}

A father offers his young daughter two toys. One---a ball---is familiar, while the other---a bright yellow feather duster---is not. After she accepts the toys, he remarks, ``Isn’t the zem funny?'' To learn the word ``zem,'' the child must determine his mother’s intended referent from a wide range of possible targets, including the two salient objects. Many factors can be useful in making this inference, including the contrast with a known object \cite{markman1988,clark1988}. But in many cases, the simplest solution may be for the child to look to her father for a sign of what he is talking about \cite{baldwin1991,vaish2011}.

The ability to follow social signals like eye-gaze is a strong predictor of children's early vocabulary growth. For example, \citeA{brooks2005} found that children who followed an experimenter's gaze better before their first birthday had larger vocabularies at 18 months. Similarly, \citeA{carpenter1998} found that children's level of joint engagement (as well as the degree to which mothers followed the child's focus of attention in their labeling) predicted vocabulary growth in both language production and comprehension. These studies suggest that children's social environment plays a powerful supportive role in language learning. 

At the same time as children are beginning to learn their first words, their view of the world is changing radically \cite{adolph2007}. As speechless infants, they are unable to locomote independently and are moved from place to place and set in particular postures by their caregivers. Before their first birthday, they begin crawling; soon after, they begin to walk independently. These changes may have a profound effect on what children see.

A recent study suggests the possibility of links between motor milestones, social cognition, and language. \citeA{walleunderreview} noted robust correlations between children's ability to walk and their vocabulary, both receptive and productive. On the basis of an observational study of parent input, they speculated that the emergence of walking may change the ability of the child to use and appreciate a number of social cues. One possible explanation for this pattern is that children's posture may mediate their access to social cues, and seeing more social information may in turn allow children to discover word meanings more effectively.

Recent methodological developments provide data that allow this hypothesis to be tested. The rise of head-mounted cameras and eye-trackers allow for measurement of children's naturalistic environment in a way that was not previously possible. \citeA{yoshida2008} gave the first demonstration of the radical differences between toddler and adult perspectives on the social world, with toddlers' visual field being dominated by hands and objects much more than adults'. More recent work has used head-mounted eye-tracking methods to measure young toddlers' fixations \cite{franchak2011}, also finding that children look relatively infrequently at their mothers' faces in naturalistic play.

These methods are now being applied to understand inputs to language acquisition. Work by Yu, Smith, and colleagues has suggested that when parents or children create moments in which the visual field is dominated by a single object and that object is named, word learning and retention is facilitated for toddlers \cite{smith2011,yuinpress}. Some data even suggest that young children's restricted viewpoint may be more effective for learning words than the comparable adult perspective\cite{yurovskyinpress}. Together, this broader body of evidence suggests that understanding infants' perspective during language use---and how it interacts with their development---is a critical part of understanding language learning.

In the current study we took a developmental approach to understanding the relationship between perspective and language input. We recorded head-camera data from a group of infants and children across a broad development range (4 -- 20 months) as they played with their caregivers during a brief laboratory visit. We then hand annotated these data for both the child's posture and the naming instances provided by the parents and used face-detection algorithms to measure the frequency and size of faces in the child's visual field.  The resulting dataset allows us to analyze both changes in access to faces according to age and posture and whether access to faces is related to naming of objects during play. 

\section{Methods}

\subsection{Participants}

Participants were 20 infants and children (N=4 each at 4, 8, 12, 16, and 20 months, 9 females total), recruited from the surrounding community via state birth records. All participants had no documented disabilities and were reported to hear at least 80\% English at home. Overall, XYZ children visited the lab, with XYZ contributing significant data. Of these, XYZ\% wore the camera successfully (with success rates at different ages varying from 100\% at 4 months to approximately XYZ\% at 20 months). Our current sample of participants were selected randomly to comprise a an age-balanced sample from the total group of participants in the ongoing study.

% The participants in our study are twenty infants whom we invited to the Social Cognition lab for hour-long sessions. The infants fall into five age groups (with four infants in each group): 4-, 8-, 12-, 16-, and 20-month-olds. Using the Center for Infant Studies (CIS) database, we screen for participants who are in the appropriate age ranges and have no documented developmental disabilities.

\subsection{Head-mounted camera}

\begin{figure}
\centering
\includegraphics[width=2in]{figures/headcam_w_fisheye3.jpg}
\caption{\label{fig:headcam} Our light-weight, low-cost head-mounted camera with fisheye lens.} 
\end{figure}

Our head-mounted camera (``headcam'') is composed of a small, inexpensive MD80 camera attached to a soft elastic headband from a camping headlamp. An aftermarket fisheye lens intended for iPhones and other Apple devices is attached to increase view angle. The total cost of each camera is approximately \$60. The camera captures 720x480 pixel images at approx. 25 frames per second, and has battery life of 60--90 minutes. With the fisheye lens affixed, it has a viewing angle of approx. 60 degrees of visual angle in the vertical axis and approx. XYZ degrees in the horizontal. The device is pictured in Figure \ref{fig:headcam}.

The vertical field of view of the camera was smaller than the child's approximate vertical field of view, which---even at 6-7 months---spans around 100--120 degrees \cite{mayer1988,cummings1988}. We were therefore faced with a choice in the orientation of the camera. If we chose a lower or higher orientation, we would be at risk of truncating either the child's own hands and physically proximate objects, or the faces of the adults around the child. Yet if we chose the middle orientation, we would still be at risk of underestimating the proportion of faces viewed by the child. Thus, for the purposes of the current study---measuring visual access to faces---we chose to orient the camera towards the upper part of the visual field.\footnote{Previous studies have shown that children's head movements in the horizontal dimension are approximated by (though are slightly lagged by) their head movements \cite{yoshida2008}. Our own experience with the current apparatus ratifies these conclusions for the horizontal field but suggests that head movements in the vertical field are less reliable.} While this orientation decreased our chances of recording the objects being manipulated by the child, it nevertheless allowed us to capture the majority of the faces in the child's visual field.

\subsection{Procedure}



After coming to the lab, families were seated in our waiting room where they signed consent documents and where children were fitted with the headcam. After a short period of play, they were escorted to a playroom in the lab where the free-play session (the focus of the current study) was conducted. 

In the waiting room, the experimenter placed the headcam on children's heads after they had time to adjust to the environment. For children who resisted wearing the headcam, the experimenter used ``distractor'' techniques (presenting stimulating toys or engaging the children in hand-occupying activities) intended to keep children's focus elsewhere and prevent them from taking off the camera \cite{yoshida2008}. Once the child was wearing the camera comfortably for a period of time, child and caregiver or caregivers (in two cases, there were two adults present) were escorted to the playroom. 

In the playroom, the experimenter presented the child's parent with a box containing three labeled pairs of objects, each consisting of a familiar and a novel object: a ball and a feather duster (``zem''), a toy car and an XYZ (blah) and an XYZ and a (blah). Parents confirmed that the child had not previously seen the novel toys. Parents were instructed to play with the object pairs with the child, one at a time, ``as they typically would'' and to use the novel labels to refer to the three toys. After giving these instructions, the experimenter left the room for a period of approximately 15 minutes. During this time, a tripod-mounted camera recorded video from a corner of the room and the headcam also captured video from the child's perspective. 

\subsection{Data Processing and Annotation}



\begin{figure*}
\includegraphics[width=7in]{figures/framesample.pdf}
\caption{\label{fig:frames} Sample frames from the headcam videos for a child from each age group, selected because they featured successful face detections (green squares).} 
\end{figure*}


All headcam videos were cropped to exclude the period of entry to the playroom and any points at which the camera needed to be adjusted and were automatically synchronized with the tripod-mounted videos using FinalCut Pro Software. The final sample was approx. 5 hours of headcam video (M=12m, range: 2--12m), for a total of roughly 400,000 frames. 

\subsubsection{Posture and Orientation Annotation}


One major goal of our study was to understand the relationship between children's posture and their access to information from the faces of their caregiver. To investigate this relationship, we created a set of hand annotations for the child's physical posture (e.g. standing, sitting) and orientation relative to the caregiver (e.g. in front of, behind, close, far away). For each headcam video, a coder used OpenSHAPA software to annotate both orientation and posture \cite{adolph2012}. 

Orientation was initially categorized as being in front of the caregiver, to the side, or behind, and close (defined informally as within arm's reach) or farther away. Because of data sparsity, we consolidated this scheme into three categories: close to the caregiver either in front or on the side, farther from the caregiver either in front or to the side, and a global category of behind the caregiver. Posture was categorized as being held/carried, lying down, sitting, crawling, standing, or other. Data from when the child was out of view of the tripod camera was marked as uncodable and excluded from these annotations. A second coder coded XYZ videos; their categorizations were reliable at XYZ. 

\subsubsection{Labeling Annotation}

We were also interested in the availability of social information proximate to naming events in the caregivers' speech to children. Accordingly, a human coder also marked the instance when the name of any of the six objects in the object set was used. Overall, caregivers produced a median of 35 labels in a highly skewed distribution across participants (range: 9 -- 131), distributed across novel and familiar objects.

\subsubsection{Face Detection}

A third goal was to measure the presence of caregivers' faces in the child's field of view (as approximated by the headcam). In order to avoid hand-annotating the size and position of faces in every frame of video, we used an in-house face-detection framework based on freely available tools \cite{bradski2008}. This system is described in depth in our previous work \cite{frank2012b} but we review it briefly here.

Our system has two parts. The first is the application of a set of four Haar-style face detection filters from the OpenCV library \cite{viola2004} to each frame of the videos independently. These detectors each provide information about whether a face is present in the frame as well as size and position for any detections. In a second step, these detections are then combined via a hidden Markov model (HMM), trained via annotated data. The intuition behind the HMM model is that a frame is much more likely to contain a face if the previous frame also contained one. The HMM model (which performed nearly as well as the more complex and computationally-intensive Conditional Random Field model used in our previous work) attempted to estimate whether a face was truly present in each frame of the videos, using as its input the number of Haar detectors that were active in any given frame. For training we annotated whether the individual face detectors were correct for XYZ XYZ. 

We additionally computed a baseline model which simply assumed that the largest detector ... 

Results for these models are reported in Table \ref{tab:results}. The HMM model obtained a relatively high level of performance.  ... Overall, the goal of our use of face-detection algorithms was to provide a measurement technique that eliminated tedious and expensive hand-coding and provided acceptable results. We therefore selected the highest performing algorithm (the XYZ) and use detections from this algorithm as an estimate of face presence in all further analyses. Sample frames from the video with overlaid detections are given in Figure \ref{fig:frames}.

\begin{table}[t]
  \caption{Model performance on gold standard generalization training set dataset. \label{tab:results} } 
  \begin{center} 
    \begin{tabular}{l|ccc} 
      \hline 
      \null & Precision & Recall & F-score  \\ 
      \hline 
      Baseline & & & \\
      HMM &        0.74  & 0.78 &    0.76   \\
      DLT & & & \\      
    \hline 
    \end{tabular} 
  \end{center}
\end{table}


\section{Results}


\begin{figure*}[t]
\includegraphics[width=3.5in]{figures/posture.pdf}
\includegraphics[width=3.5in]{figures/orientation.pdf}
\caption{\label{fig:posture} Left: Proportion time in each posture, plotted by child's age. Right: Proportion time in each orientation relative to the caregiver, again plotted by child's age. For clarity, the ``other'' code is not plotted in either figure. Error bars show standard error of the mean across participants.} 
\end{figure*}

We report results from three different sets of analyses. First, we explore developmental changes in posture and orientation in our dataset. Next, we explore how these changes affect access to faces, as measured using our face-detection algorithm. Finally, we look at access to faces during labeling events. Although this rich dataset will support a large number of additional exploratory analyses, these analyses are motivated by theoretical hypotheses. 

\subsection{Changes in Posture and Orientation}
 
Our posture coding captured typical developmental milestones (Figure \ref{fig:posture}, left). Overall, sitting was the most common posture for interactions in the caregiver play session. The youngest infants in our sample mostly sat (with parental assistance), but also lay down and were carried a significant proportion of the time. The 12-month-olds were the only group who spent a large amount of time crawling, and the 16- and 20-month-olds sat and stood in equal parts. 

% \begin{figure}[t]

% \caption{\label{fig:posture} Proportion time in each orientation by child's age. Error bars show standard error of the mean across participants.} 
% \end{figure}

Similarly, our coding of orientation revealed some significant developmental changes (Figure \ref{fig:posture}, right). Younger children more frequently had the caregiver behind them, often because the caregiver was supporting the child's sitting posture (for the 4-month-olds especially). In contrast, the 12--20 month olds were able to locomote independently and so were able to spend more time further from the caregiver. 


\subsection{Access to Faces by Posture and Orientation}


We next investigated the effects of the child's posture and orientation on the presence and size of the caregiver's face in the visual field. Figure \ref{fig:faces_by_posture} shows the proportion of frames with a positive face detection, plotted by the child's posture and orientation relative to the caregiver. Faces were seen most consistently when children were furthest from the caregiver, especially when the child was sitting or standing.\footnote{Since orientation was coded via body posture, faces seen while standing behind the caregiver are presumably due to caregivers turning their heads to look at children who have run behind them.} Since this combination of orientation and posture was primarily accessible to the toddlers, that meant that overall, older children tended to see more faces. In contrast, when young children were sitting close to (but not behind) their caregiver, they were often looking at the parent's hands and the objects they were manipulating. Crawling and lying down afforded almost no opportunity to see caregivers' face. 


% \begin{figure*}
% \centering
% \includegraphics[width=7in]{figures/size_by_posture.pdf}
% \caption{\label{fig:sizse_by_posture} Proportion of frames with a face detected, plotted by child's age, posture (line and point color), and orientation with respect to caregiver (panel). Plotting conventions are as in Figure \ref{fig:faces_by_posture}.} 
% \end{figure*}

In addition to differences in the accessibility of faces, we also observed differences in the size of the faces that were observed based on the child's posture and orientation. The largest faces were seen when children were standing close to their caregivers (who were most often sitting on the floor during the play sessions), and faces were overall smaller when the children were further away. 


Interaction between adults is often conducted at a greater distance than interaction between caregivers and children. This simple fact means that often young children have limited and non-canonical views of their caregivers' faces. It remains to be investigated whether young children can nevertheless extract information in these situations. 


\subsection{Access to Faces during Labeling}


\begin{figure*}
\centering
\includegraphics[width=6.5in]{figures/faces_by_posture.pdf}
\caption{\label{fig:faces_by_posture} Proportion of frames with a face detected, plotted by child's age, posture (line and point color), and orientation with respect to caregiver (panel). No error bars are shown due to the small and uneven distribution of data across points.} 
\end{figure*}

Our final analysis concerned the accessibility of caregivers' faces during labeling events. \citeA{franchak2011} found that referential speech was marginally more likely to draw toddlers' attention to mothers' faces. We were interested in whether looking at faces occurred during labeling, and especially whether this relationship was related to the type of labeling that occurred. Accordingly, we used the labeling annotations for each child to identify the 2s before and after each labeling event. [Why this window? Check other windows, plus add justification.] We then computed the proportion face detections within this window across ages, object types (familiar vs. novel), and whether this was the first instance of labeling for this object. 

Figure \ref{fig:naming_face} shows the results of this analysis. Overall we saw developmental increases in the amount of faces detected during labeling, consistent with the overall pattern of greater face accessibility for older children (who were after all more often sitting or standing further away from their caregiver). An interesting pattern emerged, however, when we broke down the data by label type and labeling instance: we found that there were more face detections around the first labeling instance for novel items [STATS].

This result could be caused by children, caregivers, or a combination of the two. Older toddlers could be more interested in the novel objects and could seek out caregivers (who are initially holding the objects) to find out more; or caregivers could wait until the child can see what is being talked about before introducing a new object. Most likely, both of these explanations is true in part. [SAY MORE].

\begin{figure*}
\centering
\includegraphics[width=6in]{figures/naming_face.pdf}
\caption{\label{fig:naming_face} Proportion of faces detected in a 4s window of time centered around labeling events, split by labeling instance and whether the word was familiar or novel. Error bars show standard error of the mean.} 
\end{figure*}


\section{General Discussion}

Using a head-mounted camera, we explored the relationship between infants' postural and locomotor development and their visual access to social information. The use of automated annotation tools from computer vision allowed us to measure the prevalence and size of caregivers' faces in their children's visual field. We found systematic differences in the visual accessibility of faces based on the child's orientation and posture, variables that were further mediated by age. Put simply, older children, who could move themselves, got to choose where they were and when they saw their caregivers' face. Accordingly, they saw faces more around the introductory labeling of novel, engaging objects. 

Our study complements a wide variety of work that has investigated the relationship between children's perspective and their access to social and referential information \cite{yoshida2008,franchak2011,smith2011,yuinpress,walleunderreview}. 

The measures developed here have broad applicability to the study of individual and cultural differences. Since the physical circumstances of child rearing vary widely across households and across cultures with different material circumstances, there may be predictable differences in children's visual experience. As suggested by the correlations between walking and vocabulary \cite{walleunderreview}, shifts in how infants are placed in particular postures by strollers or carriers \cite{zeedyk2008} or how much exercise they are given \cite{bril1986}.

While infants' visual field is often subject to the whims of their caregivers, toddlers determine their own input to a much greater degree. Toddlers live in a world populated by knees. [FINISH]


\section{Acknowledgments}

Thanks to Ally Kraus, Kathy Woo, Aditi Maliwal, and other members of the Language and Cognition Lab for help in recruitment, data collection, and annotation. This research was supported by a John Merck Scholars grant to MCF.

\bibliographystyle{apacite}

\setlength{\bibleftmargin}{.125in} \setlength{\bibindent}{-\bibleftmargin}

\bibliography{postureface.bib}

\end{document}

